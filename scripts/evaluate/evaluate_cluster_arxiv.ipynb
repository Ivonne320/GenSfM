{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycolmap\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# from tabilize import *\n",
    "\n",
    "# import evo.core.lie_algebra as lie\n",
    "# from evo.core import trajectory\n",
    "# from evo.tools import plot, file_interface, log\n",
    "# from evo.core import lie_algebra, sync, metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.offline import plot\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage import graph\n",
    "# import sklearn.cluster\n",
    "\n",
    "# from sklearn import metrics\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from helpers import *\n",
    "import locale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankify(x, order):\n",
    "    # Turn a vector of values into a list of ranks, while handling ties.\n",
    "    assert len(x.shape) == 1\n",
    "    if order == 0:\n",
    "        return np.full_like(x, 1e5, dtype=np.int32)\n",
    "    u = np.sort(np.unique(x))\n",
    "    if order == 1:\n",
    "        u = u[::-1]\n",
    "    r = np.zeros_like(x, dtype=np.int32)\n",
    "    for ui, uu in enumerate(u):\n",
    "        mask = x == uu\n",
    "        r[mask] = ui\n",
    "    return np.int32(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the available datasets here\n",
    "datasets_full = {\n",
    "    \"KITTI\":\n",
    "        # From IMU. The error for the 00 is too large and 03 is missing\n",
    "        [\"01\", \"02\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"],\n",
    "        # [\"01\", \"02\", \"04\", \"05\", ],\n",
    "        # [\"01\",\"04\"],\n",
    "    \"lamar\": [\n",
    "            \"2022-01-18-12-41-23-216\", \n",
    "            \"2022-06-20-14-33-15-200\",\n",
    "            \"2022-08-30-12-11-39-585\",\n",
    "            \"2022-08-30-14-37-23-164\", \n",
    "        ],\n",
    "    \"1dsfm_colmap\": [\n",
    "            'Alamo',\n",
    "            # 'Gendarmenmarkt',\n",
    "            'Madrid_Metropolis',\n",
    "            'Roman_Forum',\n",
    "            'Tower_of_London',\n",
    "        ],\n",
    "    \"euroc\" : [\n",
    "            \"MH_01_easy\",\n",
    "            \"MH_02_easy\",\n",
    "            \"MH_03_medium\",\n",
    "            \"MH_04_difficult\",\n",
    "            \"MH_05_difficult\",\n",
    "            \"V1_01_easy\",\n",
    "            \"V1_02_medium\",\n",
    "            \"V1_03_difficult\",\n",
    "            \"V2_01_easy\",\n",
    "            \"V2_02_medium\",\n",
    "            \"V2_03_difficult\"\n",
    "    ],\n",
    "    \"lamar_map\" : [\n",
    "            # \"CAB\",\n",
    "            # \"HGE\",\n",
    "            \"LIN\"\n",
    "    ],\n",
    "    \"eth3d_slam\": [\n",
    "        \"cables_1\",\n",
    "        \"cables_2\",\n",
    "        \"cables_3\",\n",
    "        \"camera_shake_1\",\n",
    "        \"camera_shake_2\",\n",
    "        \"camera_shake_3\",\n",
    "        \"ceiling_1\",\n",
    "        \"ceiling_2\",\n",
    "        \"desk_3\",\n",
    "        \"desk_changing_1\",\n",
    "        \"einstein_1\",\n",
    "        \"einstein_2\",\n",
    "        \"einstein_dark\",\n",
    "        \"einstein_flashlight\",\n",
    "        \"einstein_global_light_changes_1\",\n",
    "        \"einstein_global_light_changes_2\",\n",
    "        \"einstein_global_light_changes_3\",\n",
    "        \"kidnap_1\",\n",
    "        # \"kidnap_dark\",\n",
    "        \"large_loop_1\",\n",
    "        \"mannequin_1\",\n",
    "        \"mannequin_3\",\n",
    "        \"mannequin_4\",\n",
    "        \"mannequin_5\",\n",
    "        \"mannequin_7\",\n",
    "        \"mannequin_face_1\",\n",
    "        \"mannequin_face_2\",\n",
    "        \"mannequin_face_3\",\n",
    "        \"mannequin_head\",\n",
    "        \"motion_1\",\n",
    "        \"planar_2\",\n",
    "        \"planar_3\",\n",
    "        \"plant_1\",\n",
    "        \"plant_2\",\n",
    "        \"plant_3\",\n",
    "        \"plant_4\",\n",
    "        \"plant_5\",\n",
    "        # \"plant_dark\",\n",
    "        \"plant_scene_1\",\n",
    "        \"plant_scene_2\",\n",
    "        \"plant_scene_3\",\n",
    "        \"reflective_1\",\n",
    "        \"repetitive\",\n",
    "        \"sfm_bench\",\n",
    "        \"sfm_garden\",\n",
    "        \"sfm_house_loop\",\n",
    "        \"sfm_lab_room_1\",\n",
    "        \"sfm_lab_room_2\",\n",
    "        \"sofa_1\",\n",
    "        \"sofa_2\",\n",
    "        \"sofa_3\",\n",
    "        \"sofa_4\",\n",
    "        # \"sofa_dark_1\",\n",
    "        # \"sofa_dark_2\",\n",
    "        # \"sofa_dark_3\",\n",
    "        # \"sofa_shake\",\n",
    "        \"table_3\",\n",
    "        \"table_4\",\n",
    "        \"table_7\",\n",
    "        \"vicon_light_1\",\n",
    "        \"vicon_light_2\"\n",
    "    ],\n",
    "\n",
    "    \"eth3d_dslr\": [\n",
    "        \"courtyard\",\n",
    "        \"delivery_area\",\n",
    "        \"electro\",\n",
    "        \"facade\",\n",
    "        \"kicker\",\n",
    "        \"meadow\",\n",
    "        \"office\",\n",
    "        \"pipes\",\n",
    "        \"playground\",\n",
    "        \"relief\",\n",
    "        \"relief_2\",\n",
    "        \"terrace\",\n",
    "        \"terrains\"\n",
    "    ],\n",
    "    \"eth3d_rig\" : [\n",
    "        \"delivery_area\",\n",
    "        \"electro\",\n",
    "        \"forest\",\n",
    "        \"playground\",\n",
    "        \"terrains\"\n",
    "    ],\n",
    "    'mip360': [\n",
    "        \"bicycle\",\n",
    "        \"bonsai\",\n",
    "        \"counter\",\n",
    "        \"garden\",\n",
    "        \"kitchen\",\n",
    "        \"room\",\n",
    "        \"stump\"\n",
    "    ],\n",
    "    'eth3d_dslr_test': [\n",
    "        \"botanical_garden\",\n",
    "        \"boulders\",\n",
    "        \"bridge\",\n",
    "        \"door\",\n",
    "        \"exhibition_hall\",\n",
    "        \"lecture_room\",\n",
    "        \"living_room\",\n",
    "        \"lounge\",\n",
    "        \"observatory\",\n",
    "        \"old_computer\",\n",
    "        \"statue\",\n",
    "        \"terrace_2\",\n",
    "    ],\n",
    "    'imc_haiper': [\n",
    "        \"bike\",\n",
    "        \"chairs\",\n",
    "        \"fountain\"\n",
    "    ],\n",
    "    'imc_heritage': [\n",
    "        \"cyprus\",\n",
    "        \"dioscuri\",\n",
    "        \"wall\"\n",
    "    ],\n",
    "    'imc_urban': [\n",
    "        \"kyiv-puppet-theater\"\n",
    "    ],\n",
    "    'imc_phototourism': [\n",
    "        'brandenburg_gate',\n",
    "        'british_museum',\n",
    "        'buckingham_palace',\n",
    "        'colosseum_exterior',\n",
    "        'grand_place_brussels',\n",
    "        'lincoln_memorial_statue',\n",
    "        'notre_dame_front_facade',\n",
    "        'pantheon_exterior',\n",
    "        'piazza_san_marco',\n",
    "        'sacre_coeur',\n",
    "        'sagrada_familia',\n",
    "        'st_pauls_cathedral',\n",
    "        'st_peters_square',\n",
    "        'taj_mahal',\n",
    "        # 'temple_nara_japan',\n",
    "        'trevi_fountain'\n",
    "    ],\n",
    "    '1dsfm_ori': [\n",
    "        # \"ArtsQuad\",\n",
    "        # \"Union_Square\",\n",
    "        \"Vienna_Cathedral\"\n",
    "    ],\n",
    "    'flowmap': [\n",
    "        \"co3d_bench\",\n",
    "        \"co3d_hydrant\",\n",
    "        \"llff_fern\",\n",
    "        \"llff_flower\",\n",
    "        \"llff_fortress\",\n",
    "        \"llff_horns\",\n",
    "        # \"llff_leaves\",\n",
    "        \"llff_orchids\",\n",
    "        \"llff_room\",\n",
    "        \"llff_trex\",\n",
    "        \"mipnerf360_bonsai\",\n",
    "        \"mipnerf360_counter\",\n",
    "        # \"mipnerf360_garden\",\n",
    "        \"mipnerf360_kitchen\",\n",
    "        # \"tandt_auditorium\",\n",
    "        # \"tandt_ballroom\",\n",
    "        \"tandt_barn\",\n",
    "        \"tandt_caterpillar\",\n",
    "        \"tandt_church\",\n",
    "        \"tandt_courthouse\",\n",
    "        # \"tandt_courtroom\",\n",
    "        \"tandt_family\",\n",
    "        \"tandt_francis\",\n",
    "        \"tandt_horse\",\n",
    "        \"tandt_ignatius\",\n",
    "        # \"tandt_lighthouse\",\n",
    "        \"tandt_m60\",\n",
    "        # \"tandt_meetingroom\",\n",
    "        \"tandt_museum\",\n",
    "        # \"tandt_palace\",\n",
    "        \"tandt_panther\",\n",
    "        \"tandt_playground\",\n",
    "        # \"tandt_temple\",\n",
    "        \"tandt_train\",\n",
    "        \"tandt_truck\"\n",
    "    ],\n",
    "    \"strecha\": [\n",
    "        \"Castle\",\n",
    "        \"Castle_large\",\n",
    "        \"Entry\",\n",
    "        \"Fountain\",\n",
    "        \"Herzjesu\",\n",
    "        \"Herzjesu_large\",\n",
    "    ],\n",
    "    \"imc2024\": [\n",
    "        # \"church\",\n",
    "        \"dioscuri\",\n",
    "        # \"lizard\",\n",
    "        \"multi-temporal-temple-baalshamin\",\n",
    "        # \"pond\",\n",
    "        # \"transp_obj_glass_cup\",\n",
    "        # \"transp_obj_glass_cylinder\"\n",
    "    ],\n",
    "    \"imc2021_test\": [\n",
    "        \"british_museum\",\n",
    "        \"florence_cathedral_side\",\n",
    "        \"lincoln_memorial_statue\",\n",
    "        \"london_bridge\",\n",
    "        \"milan_cathedral\",\n",
    "        \"mount_rushmore\",\n",
    "        \"piazza_san_marco\",\n",
    "        \"sagrada_familia\",\n",
    "        \"st_pauls_cathedral\"\n",
    "    ],\n",
    "    \"imc2021_val\": [\n",
    "        \"reichstag\",\n",
    "        \"sacre_coeur\",\n",
    "        \"st_peters_square\"\n",
    "    ],\n",
    "    \"eth3d_raw_dslr\": [\n",
    "        \"courtyard\",\n",
    "        # \"delivery_area\",\n",
    "        # \"electro\",\n",
    "        \"facade\",\n",
    "        # \"kicker\",\n",
    "        # \"meadow\",\n",
    "        # \"office\",\n",
    "        # \"pipes\",\n",
    "        # \"playground\",\n",
    "        # \"relief\",\n",
    "        # \"relief_2\",\n",
    "        # \"terrace\",\n",
    "        # \"terrains\"\n",
    "    ],\n",
    "    'eth3d_raw_dslr_test': [\n",
    "        \"botanical_garden\",\n",
    "        \"boulders\",\n",
    "        \"bridge\",\n",
    "        \"door\",\n",
    "        \"exhibition_hall\",\n",
    "        \"lecture_room\",\n",
    "        \"living_room\",\n",
    "        \"lounge\",\n",
    "        \"observatory\",\n",
    "        \"old_computer\",\n",
    "        \"statue\",\n",
    "        \"terrace_2\",\n",
    "    ],\n",
    "    'blendedmvs': None,\n",
    "    'blendedmvs+': None,\n",
    "    'blendedmvs++': None,\n",
    "}\n",
    "\n",
    "times_colmap = {root: {} for root in datasets_full.keys()}\n",
    "times_colmap[\"euroc\"] = {\n",
    "    \"MH_01_easy\" : 48914.0,\n",
    "    \"MH_02_easy\" : 51894.24,\n",
    "    \"MH_03_medium\" : 24453.3,\n",
    "    \"MH_04_difficult\" : 13708.62,\n",
    "    \"MH_05_difficult\" : 4525.68,\n",
    "    \"V1_01_easy\" : 37055.76,\n",
    "    \"V1_02_medium\" : 3269.04,\n",
    "    \"V1_03_difficult\" : 2410.5,\n",
    "    \"V2_01_easy\" : 9367.32,\n",
    "    \"V2_02_medium\" : 3759.18,\n",
    "    \"V2_03_difficult\" : 429.06,\n",
    "}\n",
    "times_colmap['lamar_map'] = {\n",
    "    \"CAB\": 194033.6,\n",
    "    \"HGE\": 249771.1,\n",
    "    \"LIN\": 620176.38\n",
    "}\n",
    "\n",
    "\n",
    "reconstruction_file_names = [\"cameras.bin\", \"images.bin\", \"points3D.bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base dir here\n",
    "path_roots = {\n",
    "    \"euroc\":\n",
    "        \"../../data/euroc/\",\n",
    "    \"1dsfm_colmap\":\n",
    "        \"../../data/1dsfm_colmap/\",\n",
    "    \"KITTI\":\n",
    "        \"../../data/KITTI/dataset/processed/\",\n",
    "    \"eth3d_slam\":\n",
    "        \"../../data/ETH3D/training/\",\n",
    "    \"eth3d_dslr\":\n",
    "        \"../../data/ETH3D/mvs/multi_view_training_dslr_undistorted/\",\n",
    "    \"eth3d_rig\":\n",
    "        \"../../data/ETH3D/mvs/multi_view_training_rig_undistorted/\",\n",
    "    \"mip360\":\n",
    "        \"../../data/mip360/\",\n",
    "    \"lamar_map\":\n",
    "        \"../../data/lamar_map/\",\n",
    "    \"1dsfm_ori\":\n",
    "        \"../../data/1dsfm_ori/\",\n",
    "    \"1dsfm\":\n",
    "        \"../../data/1dsfm/datasets/\",\n",
    "    \"eth3d_dslr_test\":\n",
    "        \"../../data/ETH3D/mvs/multi_view_test_dslr_undistorted/\",\n",
    "    \"imc_haiper\":\n",
    "        \"../../data/IMC2023/train/haiper/\",\n",
    "    \"imc_heritage\":\n",
    "        \"../../data/IMC2023/train/heritage/\",\n",
    "    \"imc_urban\":\n",
    "        \"../../data/IMC2023/train/urban/\",\n",
    "    \"imc_phototourism\":\n",
    "        \"../../data/IMC2023/train/phototourism/\",\n",
    "    \"strecha\":\n",
    "        \"../../data/strecha/\",\n",
    "    \"imc2024\":\n",
    "        \"../../data/IMC2024/train/\",\n",
    "    \"imc2021_test\":\n",
    "        \"../../data/IMC2021_test/phototourism/\",\n",
    "    \"imc2021_val\":\n",
    "        \"../../data/IMC2021_val/phototourism/\",\n",
    "    \"eth3d_raw_dslr\":\n",
    "        \"../../data/ETH3D/mvs/multi_view_training_dslr_jpg/\",\n",
    "    \"eth3d_raw_dslr_test\":\n",
    "        \"../../data/ETH3D/mvs/multi_view_test_dslr_jpg/\",\n",
    "    \"blendedmvs\":\n",
    "        \"../../../datasets/BlendedMVS/\",\n",
    "    \"blendedmvs+\":\n",
    "        \"../../../datasets/BlendedMVS+/\",\n",
    "    \"blendedmvs++\":\n",
    "        \"../../../datasets/BlendedMVS++/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change the configuration here\n",
    "# euroc, mip360, eth3d_slam, eth3d_dslr, eth3d_dslr_test, eth3d_rig, imc_haiper, imc_heritage, imc_urban, imc_phototourism\n",
    "# roots = [\"mip360\"]\n",
    "# roots = [\"lamar_map\"]\n",
    "# roots = [\"eth3d_rig\"]\n",
    "# roots = [\"strecha\"]\n",
    "# roots = [\"eth3d_slam\"]\n",
    "# roots = [\"eth3d_dslr\", \"eth3d_dslr_test\"]\n",
    "# roots = [\"imc_haiper\", \"imc_heritage\", \"imc_urban\", \"imc_phototourism\"]\n",
    "# roots = [\"imc_haiper\", \"imc_heritage\", \"imc_urban\"]\n",
    "# roots = [\"imc_heritage\"]\n",
    "# roots = [\"imc_haiper\"]\n",
    "# roots = [\"imc_phototourism\"]\n",
    "# roots = [\"imc2021_test\"]\n",
    "# roots = [\"imc2024\"]\n",
    "# roots = [\"eth3d_dslr\"]\n",
    "roots = [\"eth3d_raw_dslr\"]\n",
    "# roots = [\"eth3d_raw_dslr\", \"eth3d_raw_dslr_test\"]\n",
    "# roots = [\"euroc\"]\n",
    "# roots = [\"blendedmvs\"]\n",
    "\n",
    "\n",
    "\n",
    "# 3d, colmap\n",
    "# post_fix = \"bundled_iterative\"\n",
    "post_fix = \"refactored\"\n",
    "# post_fix = \"refactored_sparse\"\n",
    "# post_fix = \"bundled_pipeline\"\n",
    "# post_fix = \"test_gtrot\"\n",
    "# post_fix = \"test_calibed_largermerge\"\n",
    "\n",
    "\n",
    "path_bases = [\n",
    "    # \"colmap_openmvg/reconstruction\",\n",
    "    # # # # \"globalsfm_lud/aligned_cauchy\", # for imc_haiper, imc_heritage, imc_urban, imc_phototourism, eth3d_dslr, eth3d_dslr_test\n",
    "    # # # # # \"globalsfm_lud/aligned\", # for eth3d_slam, eth3d_rig, mip360\n",
    "    # # \"globalsfm_theia\",\n",
    "    # # # \"globalsfm_theia_test\",\n",
    "    # # \"globalsfm_hsfm\",\n",
    "    # # # \"globalsfm_nonlinear/test_calibed\",\n",
    "    # # # \"test/5\",\n",
    "    # # # \"test_ra/5\",\n",
    "    # # # \"globalsfm_nonlinear/test_calibed_noreest\",\n",
    "    # # # \"globalsfm_nonlinear/test_calibed_oriF\",\n",
    "    # # # \"globalsfm_nonlinear/pts_test/0\",\n",
    "    # # # \"globalsfm_nonlinear/test\",\n",
    "    # # \"globalsfm_nonlinear/bundled_pipeline\",\n",
    "    # # \"globalsfm_nonlinear/bundled_iterative\",\n",
    "    # # # \"globalsfm_nonlinear/test_gtrot\",\n",
    "    # # \"globalsfm_nonlinear/\" + post_fix + \"_notriang\",\n",
    "    # # \"globalsfm_theia\",\n",
    "    # \"globalsfm_theia_refactored\",\n",
    "    # \"globalsfm_hsfm_refactored\",\n",
    "    # \"colmap_ligt/reconstruction\",\n",
    "    # # \"globalsfm_hsfm\",\n",
    "    # # \"globalsfm_nonlinear/\" + post_fix,\n",
    "    # # \"globalsfm_nonlinear/refactored_lud\",\n",
    "    # # \"globalsfm_nonlinear/refactored_onlycam\",\n",
    "    # # \"globalsfm_nonlinear/refactored_ptcam\",\n",
    "    # \"globalsfm_nonlinear/refactored_onlypt\",\n",
    "    \"globalsfm_nonlinear/refactored_onlypt/0\",\n",
    "    # \"globalsfm_nonlinear/nonadaptive_onlypt/0\",\n",
    "    # \"globalsfm_nonlinear/adaptive_onlypt/0\",\n",
    "    # \"globalsfm_nonlinear/rot_1dof/0\",\n",
    "    # \"globalsfm_nonlinear/rot_3dof/0\",\n",
    "    # \"globalsfm_nonlinear/ra_1dof/0\",\n",
    "    # \"globalsfm_nonlinear/ra_3dof/0\",\n",
    "    # # \"globalsfm_nonlinear/positioning_ptcam\",\n",
    "    # # \"globalsfm_nonlinear/positioning_onlypt\",\n",
    "    # # \"globalsfm_nonlinear/positioning_onlypt_new\",\n",
    "    # # \"globalsfm_nonlinear/refactored_notriang\",\n",
    "    # # \"globalsfm_nonlinear/refactored\",\n",
    "    # # \"globalsfm_nonlinear/refactored_sparse_40_notriang\",\n",
    "    # # \"globalsfm_nonlinear/refactored_sparse_40\",\n",
    "    # # \"globalsfm_nonlinear/refactored_sparse_45_notriang\",\n",
    "    # # \"globalsfm_nonlinear/refactored_sparse_45\",\n",
    "    # # \"globalsfm_nonlinear/refactored_sparse_48_notriang\",\n",
    "    # # \"globalsfm_nonlinear/bundled_pipeline\",\n",
    "    # # \"globalsfm_nonlinear/bundled_nosparsify_1ra\",\n",
    "    # # \"globalsfm_nonlinear/bundled_nosparsify\",\n",
    "    # # \"globalsfm_nonlinear/pts/empty\",\n",
    "    \"sparse/0\",\n",
    "    # \"colmap\",\n",
    "    # # \"hloc/sparse\",\n",
    "    # # \"hloc/globalsfm_nonlinear/test_calibed_oriF\",\n",
    "    # # \"sfm\",\n",
    "]\n",
    "\n",
    "# path_bases = [\n",
    "#     # \"globalsfm_nonlinear/bundled_theia\",\n",
    "#     # \"globalsfm_nonlinear/bundled_nopt\",\n",
    "#     # \"globalsfm_nonlinear/bundled_ptcam\",\n",
    "#     # \"globalsfm_nonlinear/\" + post_fix,\n",
    "#     \"globalsfm_nonlinear/refactored_lud\",\n",
    "#     \"globalsfm_nonlinear/refactored_onlycam\",\n",
    "#     \"globalsfm_nonlinear/refactored_ptcam\",\n",
    "#     \"globalsfm_nonlinear/refactored_onlypt\",\n",
    "# ]\n",
    "\n",
    "# path_bases = [\"globalsfm_nonlinear/test/0\"]\n",
    "# path_bases = [\"\"]\n",
    "# path_bases = [\"sparse_resized/0\", \"glomap_resized/0\", \"mast3r_sfm/0\"]\n",
    "# path_bases = [\"sparse/0\", \"globalsfm_nonlinear/refactored_onlypt\", \"e2esfm\"]\n",
    "# path_bases = [\"glomap/0\"]\n",
    "# path_bases = [\"globalsfm_nonlinear/glomap_stochastic_3it/0\", \"globalsfm_nonlinear/glomap_stochastic_0.5/0\", \"globalsfm_nonlinear/glomap_stochastic/0\", \"globalsfm_nonlinear/glomap_stochastic_0.1/0\", \"globalsfm_nonlinear/test/0\"]\n",
    "# path_bases = [\"globalsfm_nonlinear/weight_random_gponly/0\", \"globalsfm_nonlinear/weight_uniform_gponly/0\", \"globalsfm_nonlinear/weight_trained_gponly/0\", \"globalsfm_nonlinear/weight_inliers_gponly/0\"]\n",
    "\n",
    "path_bases = [\"colmap_thinprism/0\", \"colmap_radial/0\", \"implicit_sfm\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, 'refactored']\n",
      "[None, None, None, 'refactored']\n",
      "['colmap_thinprism/0', 'colmap_radial/0', 'implicit_sfm']\n",
      "{'KITTI': {}, 'lamar': {}, '1dsfm_colmap': {}, 'euroc': {}, 'lamar_map': {}, 'eth3d_slam': {}, 'eth3d_dslr': {}, 'eth3d_rig': {}, 'mip360': {}, 'eth3d_dslr_test': {}, 'imc_haiper': {}, 'imc_heritage': {}, 'imc_urban': {}, 'imc_phototourism': {}, '1dsfm_ori': {}, 'flowmap': {}, 'strecha': {}, 'imc2024': {}, 'imc2021_test': {}, 'imc2021_val': {}, 'eth3d_raw_dslr': {}, 'eth3d_raw_dslr_test': {}, 'blendedmvs': {}, 'blendedmvs+': {}, 'blendedmvs++': {}, 'refactored': {'KITTI': {}, 'lamar': {}, '1dsfm_colmap': {}, 'euroc': {}, 'lamar_map': {'CAB': 6162.24, 'HGE': 12587.2, 'LIN': 18466.6}, 'eth3d_slam': {'cables_1': 499.254, 'cables_2': 12.1554, 'cables_3': 26.3364, 'camera_shake_1': 7.5131, 'camera_shake_2': 11.6773, 'camera_shake_3': 2.7526, 'ceiling_1': 39.4557, 'ceiling_2': 164.105, 'desk_3': 145.981, 'desk_changing_1': 173.527, 'einstein_1': 147.958, 'einstein_2': 501.503, 'einstein_dark': 47.5182, 'einstein_flashlight': 87.1577, 'einstein_global_light_changes_1': 27.7926, 'einstein_global_light_changes_2': 101.567, 'einstein_global_light_changes_3': 151.453, 'kidnap_1': 136.633, 'large_loop_1': 73.7271, 'mannequin_1': 18.1715, 'mannequin_3': 13.3309, 'mannequin_4': 52.5712, 'mannequin_5': 75.9402, 'mannequin_7': 17.353, 'mannequin_face_1': 51.2339, 'mannequin_face_2': 83.778, 'mannequin_face_3': 51.9027, 'mannequin_head': 30.5537, 'motion_1': 654.228, 'planar_2': 548.112, 'planar_3': 596.761, 'plant_1': 4.29072, 'plant_2': 24.5841, 'plant_3': 17.6381, 'plant_4': 9.71101, 'plant_5': 17.475, 'plant_scene_1': 39.4591, 'plant_scene_2': 53.5379, 'plant_scene_3': 48.701, 'reflective_1': 448.147, 'repetitive': 86.6586, 'sfm_bench': 96.1836, 'sfm_garden': 703.263, 'sfm_house_loop': 207.36, 'sfm_lab_room_1': 34.248, 'sfm_lab_room_2': 6.23097, 'sofa_1': 15.4451, 'sofa_2': 8.44966, 'sofa_3': 4.90101, 'sofa_4': 11.7984, 'table_3': 313.41, 'table_4': 204.429, 'table_7': 181.558, 'vicon_light_1': 56.4243, 'vicon_light_2': 38.3656}, 'eth3d_dslr': {'meadow': 2.06594, 'courtyard': 22.5217, 'delivery_area': 12.2593, 'electro': 7.42349, 'facade': 80.1566, 'kicker': 6.73219, 'office': 1.11073, 'pipes': 0.785648, 'playground': 7.5085, 'relief': 15.2908, 'relief_2': 6.19774, 'terrace': 3.1898, 'terrains': 6.48447}, 'eth3d_rig': {'delivery_area': 505.127, 'electro': 362.866, 'forest': 1705.81, 'playground': 854.408, 'terrains': 394.127}, 'mip360': {'bicycle': 60.6206, 'bonsai': 463.517, 'counter': 206.727, 'garden': 135.606, 'kitchen': 435.005, 'room': 226.912, 'stump': 26.7093}, 'eth3d_dslr_test': {'botanical_garden': 3.97586, 'boulders': 10.269, 'bridge': 86.8767, 'door': 2.54045, 'exhibition_hall': 69.491, 'lecture_room': 8.10702, 'living_room': 35.1007, 'lounge': 1.28408, 'observatory': 12.2421, 'old_computer': 8.42171, 'statue': 4.38534, 'terrace_2': 3.46139}, 'imc_haiper': {'bike': 1.47088, 'chairs': 1.64408, 'fountain': 3.84643}, 'imc_heritage': {'cyprus': 2.12205, 'dioscuri': 116.058, 'wall': 65.9943}, 'imc_urban': {'kyiv-puppet-theater': 2.73331}, 'imc_phototourism': {'brandenburg_gate': 298.007, 'british_museum': 115.332, 'buckingham_palace': 498.532, 'colosseum_exterior': 804.572, 'grand_place_brussels': 241.037, 'lincoln_memorial_statue': 109.968, 'notre_dame_front_facade': 4066.85, 'pantheon_exterior': 463.791, 'piazza_san_marco': 45.9739, 'sacre_coeur': 300.354, 'sagrada_familia': 114.058, 'st_pauls_cathedral': 94.1705, 'st_peters_square': 1055.21, 'taj_mahal': 539.624, 'trevi_fountain': 1718.46}, '1dsfm_ori': {}, 'flowmap': {}, 'strecha': {}, 'imc2024': {}, 'imc2021_test': {}, 'imc2021_val': {}, 'eth3d_raw_dslr': {}, 'eth3d_raw_dslr_test': {}, 'blendedmvs': {}, 'blendedmvs+': {}, 'blendedmvs++': {}, 'Tower_of_London': 61.691}}\n"
     ]
    }
   ],
   "source": [
    "# read in time\n",
    "\n",
    "times_3d = {x: {} for x in datasets_full.keys()}\n",
    "times_lud = {x: {} for x in datasets_full.keys()}\n",
    "times_theia = {x: {} for x in datasets_full.keys()}\n",
    "times_hsfm = {x: {} for x in datasets_full.keys()}\n",
    "times_openmvg= {}\n",
    "times_ligt= {}\n",
    "\n",
    "target_string = \"globalsfm_nonlinear\"\n",
    "post_fixes = [x.split(\"/\")[1] if x is not None and x.find(target_string) >= 0 else None for x in path_bases]\n",
    "if not post_fix in post_fixes:\n",
    "    post_fixes.append(post_fix)\n",
    "\n",
    "print(post_fixes)\n",
    "\n",
    "for post_fix_temp in post_fixes:\n",
    "    if post_fix_temp is None:\n",
    "        continue\n",
    "    times_3d[post_fix_temp] = {x: {} for x in datasets_full.keys()}\n",
    "    timefile_names = [\"../../build/time_trans.txt\", \"../../build/time_trans_postsubmission.txt\", \"../../../glomap_refactor/build/time_trans_refactored.txt\"]\n",
    "    for timefile_name in timefile_names:\n",
    "        file = open(timefile_name, \"r\")\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            line = line.strip()\n",
    "            if line == post_fix_temp:        \n",
    "                line = file.readline()         \n",
    "                while line.strip() != \"\":\n",
    "                    line = line.strip()\n",
    "                    times = line.split()\n",
    "                    \n",
    "                    if times[-2] == \"1\":\n",
    "                        if len(times) == 3:\n",
    "                            times_3d[post_fix_temp][times[-3]] = float(times[-1])\n",
    "                        else:\n",
    "                            times_3d[post_fix_temp][times[0]][times[-3]] = float(times[-1])\n",
    "                    elif times[-2] == \"2\":\n",
    "                        if len(times) == 3:\n",
    "                            times_lud[times[-3]] = float(times[-1])\n",
    "                        else:\n",
    "                            times_lud[times[0]][times[-3]] = float(times[-1])\n",
    "\n",
    "                    line = file.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "\n",
    "file = open(\"time_colmap.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_colmap[line[0]][line[1]] = float(line[2])\n",
    "\n",
    "file = open(\"time_colmap_mvs.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_colmap[line[0]][line[1]] = float(line[2])\n",
    "\n",
    "file = open(\"time_colmap_mip360.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_colmap[line[0]][line[1]] = float(line[2])\n",
    "\n",
    "file = open(\"time_colmap_imc.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_colmap[line[0]][line[1]] = float(line[2])\n",
    "\n",
    "file = open(\"time_lud.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_lud[line[0]][line[1]] = float(line[-1])\n",
    "\n",
    "# file = open(\"../../build/time_theia.txt\", \"r\")\n",
    "# data = file.readlines()\n",
    "# data = [x.strip().split() for x in data]\n",
    "# for line in data:\n",
    "#     times_theia[line[0]][line[1]] = float(line[-1])\n",
    "\n",
    "# file = open(\"../../build/time_hsfm.txt\", \"r\")\n",
    "# data = file.readlines()\n",
    "# data = [x.strip().split() for x in data]\n",
    "# for line in data:\n",
    "#     times_hsfm[line[0]][line[1]] = float(line[-1])\n",
    "\n",
    "file = open(\"../../../glomap_refactor/build/time_theia_refactored.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_theia[line[0]][line[1]] = float(line[-1])\n",
    "\n",
    "file = open(\"../../../glomap_refactor/build/time_hsfm_refactored.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_hsfm[line[0]][line[1]] = float(line[-1])\n",
    "\n",
    "if (len(path_bases) > 1 and path_bases[1].find(\"cauchy\") >= 0):\n",
    "    file = open(\"time_lud_colmap_cauchy.txt\", \"r\")\n",
    "else:\n",
    "    file = open(\"time_lud_colmap.txt\", \"r\")\n",
    "    \n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    if line[0] not in times_lud:\n",
    "        times_lud[line[0]] = {}\n",
    "    if line[1] not in times_lud[line[0]]:\n",
    "        times_lud[line[0]][line[1]] = float(line[-1])\n",
    "    else:\n",
    "        times_lud[line[0]][line[1]] += float(line[-1])\n",
    "\n",
    "file = open(\"../../../kapture/openmvg_time.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_openmvg[line[1]] = float(line[2])\n",
    "    \n",
    "file = open(\"../../../kapture/ligt_time.txt\", \"r\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    times_ligt[line[1]] = float(line[2])\n",
    "\n",
    "# there is a annoying \"s\" at the end of the time, remove it\n",
    "file = open(\"../../build/time_relpose.txt\")\n",
    "data = file.readlines()\n",
    "data = [x.strip().split() for x in data]\n",
    "for line in data:\n",
    "    for post_fix_temp in post_fixes:\n",
    "        if post_fix_temp is None:\n",
    "            continue\n",
    "        if post_fix_temp.find(\"refactored\") >= 0:\n",
    "            continue\n",
    "        # if line[0] in times_3d[post_fix_temp] and line[1] in times_3d[post_fix_temp][line[0]]:\n",
    "        #     times_3d[post_fix_temp][line[0]][line[1]] += float(line[-1][:-1])\n",
    "    # if line[0] in times_theia and line[1] in times_theia[line[0]]:\n",
    "    #     times_theia[line[0]][line[1]] += float(line[-1][:-1])\n",
    "    # if line[0] in times_hsfm and line[1] in times_hsfm[line[0]]:\n",
    "    #     times_hsfm[line[0]][line[1]] += float(line[-1][:-1])\n",
    "\n",
    "\n",
    "print(post_fixes)\n",
    "print(path_bases)\n",
    "print(times_3d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m errors_r_all \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m errors_t_all \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root \u001b[38;5;129;01min\u001b[39;00m \u001b[43mroots\u001b[49m:\n\u001b[1;32m      8\u001b[0m     results[root] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m     results_r[root] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roots' is not defined"
     ]
    }
   ],
   "source": [
    "# results for the relative pose\n",
    "results = {}\n",
    "results_r = {}\n",
    "total_image = {}\n",
    "errors_r_all = []\n",
    "errors_t_all = []\n",
    "for root in roots:\n",
    "    results[root] = {}\n",
    "    results_r[root] = {}\n",
    "    total_image[root] = {}\n",
    "    if datasets_full[root] is None:\n",
    "        datasets_full[root] = sorted([x for x in os.listdir(path_roots[root]) if os.path.isdir(path_roots[root] + x)], key=locale.strxfrm)\n",
    "    for dataset in datasets_full[root]:\n",
    "        dir_base = path_roots[root] + dataset + \"/\"\n",
    "\n",
    "        if root == \"strecha\":\n",
    "            dir_base = path_roots[root] + dataset + \"/colmap/\"\n",
    "        if root == \"imc2021_test\":\n",
    "            dir_base = path_roots[root] + dataset + \"/set_100/\"\n",
    "\n",
    "        print(dataset)\n",
    "\n",
    "        results[root][dataset] = []\n",
    "        results_r[root][dataset] = []\n",
    "        database_path = os.path.join(dir_base, \"database.db\")\n",
    "        image_path = os.path.join(dir_base, \"images\")\n",
    "\n",
    "        for path_base in path_bases:\n",
    "            if path_base is None:\n",
    "                results[root][dataset].append(None)\n",
    "                results_r[root][dataset].append(None)\n",
    "                continue\n",
    "            \n",
    "            if not os.path.exists(os.path.join(dir_base, path_base, \"cameras.txt\")) and not os.path.exists(os.path.join(dir_base, path_base, \"cameras.bin\")):\n",
    "                print(os.path.join(dir_base, path_base), \"not possible\")\n",
    "                results[root][dataset].append(None)\n",
    "                results_r[root][dataset].append(None)\n",
    "                continue\n",
    "            \n",
    "            # Only use the txt files as they are directly generated\n",
    "            for name in reconstruction_file_names:\n",
    "                if os.path.exists(os.path.join(dir_base, path_base, name)) and os.path.exists(os.path.join(dir_base, path_base, name[:-4]+ \".txt\")):\n",
    "                    os.remove(os.path.join(dir_base, path_base, name))\n",
    "                    print(\"removed\", os.path.join(dir_base, path_base, name))\n",
    "                    \n",
    "            status = True\n",
    "            key_words = [\"sparse\", \"colmap_thinprism\", \"colmap_radial\"]\n",
    "            for key_word in key_words:\n",
    "                if path_base.find(key_word + \"/0\") >= 0:\n",
    "                    total_dirs = sorted([x for x in os.listdir(dir_base + key_word + \"\") if os.path.isdir(dir_base + key_word + \"/\" + x)])\n",
    "                    if len(total_dirs) > 1:\n",
    "                        # num_imgs = []\n",
    "                        max_num_img = 0\n",
    "                        max_idx = -1\n",
    "                        for idx, sub_dir in enumerate(total_dirs):\n",
    "                            reconstruction = pycolmap.Reconstruction()\n",
    "                            reconstruction.read(dir_base + key_word + \"/\" + sub_dir)\n",
    "                            if reconstruction.num_images() > max_num_img:\n",
    "                                max_num_img = reconstruction.num_images()\n",
    "                                max_idx = idx\n",
    "                        if max_idx != 0:\n",
    "                            status = False\n",
    "                            path_base = key_word + \"/\" + str(max_idx)\n",
    "                            print(\"path_base:\", path_base)\n",
    "            \n",
    "            if path_base.find(\"colmap_openmvg\") >= 0 or path_base.find(\"colmap_ligt\") >= 0:\n",
    "                reconstruction = pycolmap.Reconstruction()\n",
    "                reconstruction.read(dir_base + path_base)\n",
    "                for id in reconstruction.reg_image_ids():\n",
    "                    image_name = reconstruction.images[id].name\n",
    "                    if image_name.find(\"records_data/\") >= 0:\n",
    "                        reconstruction.images[id].name = image_name.replace(\"records_data/\", \"\")\n",
    "                reconstruction.write_text(dir_base + path_base)\n",
    "\n",
    "            file = open(dir_base + \"centers_gt.txt\", \"r\")\n",
    "            centers_gt = {}\n",
    "            names = []\n",
    "            centers = []\n",
    "            for line in file:\n",
    "                line = line.split()\n",
    "                centers_gt[line[0]] = np.array([float(line[1]), float(line[2]), float(line[3])])\n",
    "                names.append(line[0])\n",
    "                centers.append(centers_gt[line[0]])\n",
    "\n",
    "\n",
    "            file = open(dir_base + \"gtpose_trans.txt\", \"r\")\n",
    "            poses_gt = {}\n",
    "            for line in file:\n",
    "                line = line.split()\n",
    "                poses_gt[line[0]] = poselib.CameraPose()\n",
    "                poses_gt[line[0]].q = np.array([float(line[1]), float(line[2]), float(line[3]), float(line[4])])\n",
    "                poses_gt[line[0]].t = np.array([float(line[5]), float(line[6]), float(line[7])])\n",
    "\n",
    "            reconstruction = pycolmap.Reconstruction()\n",
    "            reconstruction.read(dir_base + path_base)\n",
    "\n",
    "            poses_calc = {}\n",
    "            for image_name in centers_gt.keys():\n",
    "                image = reconstruction.find_image_with_name(image_name)\n",
    "                if image is not None:\n",
    "                    pose = poselib.CameraPose()\n",
    "                    pose.q = image.cam_from_world.rotation.quat[[3,0,1,2]]\n",
    "                    pose.t = image.cam_from_world.translation\n",
    "                    poses_calc[image_name] = pose\n",
    "\n",
    "            poses_gt_list = np.array([poses_gt[x] for x in sorted(poses_gt.keys())])\n",
    "            poses_calc_list = np.array([poses_calc[x] if x in poses_calc else None for x in sorted(poses_gt.keys())])\n",
    "                \n",
    "            # centers_est = {}\n",
    "            errors_r = []\n",
    "            errors_t = []\n",
    "            pose = poselib.CameraPose()\n",
    "            image_names = sorted(list(centers_gt.keys()))\n",
    "\n",
    "            \n",
    "            errors_r_ori = np.array(errors_r)\n",
    "            errors_t_ori = np.array(errors_t)\n",
    "\n",
    "            errors_r, errors_t = calc_pairwise_relative_error_batch(poses_gt_list, poses_calc_list)\n",
    "\n",
    "            errors_r_all += list(errors_r)\n",
    "            errors_t_all += list(errors_t)\n",
    "            \n",
    "            errors_t = np.array(errors_t)\n",
    "            # errors_r = np.array(errors_r)\n",
    "            results[root][dataset].append(np.maximum(errors_r, errors_t))\n",
    "            # results[root][dataset].append(errors_r)\n",
    "            results_r[root][dataset].append(errors_r)\n",
    "            total_image[root][dataset] = len(centers_gt)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 3, 5]\n",
      "courtyard && \\cellcolor{tabfirst}55.9 & \\cellcolor{tabsecond}25.5 && \\cellcolor{tabfirst}83.0 & \\cellcolor{tabsecond}62.4 && \\cellcolor{tabfirst}89.8 & \\cellcolor{tabsecond}75.2 \\\\\n",
      "facade && \\cellcolor{tabfirst}72.8 & \\cellcolor{tabsecond}63.3 && \\cellcolor{tabfirst}88.3 & \\cellcolor{tabsecond}84.2 && \\cellcolor{tabfirst}91.9 & \\cellcolor{tabsecond}89.3 \\\\\n",
      "\\midrule\n",
      "\\textit{Average} && \\cellcolor{tabfirst}64.4 & \\cellcolor{tabsecond}44.4 && \\cellcolor{tabfirst}85.7 & \\cellcolor{tabsecond}73.3 && \\cellcolor{tabfirst}90.8 & \\cellcolor{tabsecond}82.2 && \\cellcolor{tabfirst}10000000000.0 & \\cellcolor{tabfirst}10000000000.0 \\\\\n"
     ]
    }
   ],
   "source": [
    "# printing method, color-coded\n",
    "\n",
    "roots_rel = [\"eth3d_rig\", \"mip360\", \"eth3d_dslr\", \"eth3d_dslr_test\", \"imc_haiper\", \"imc_heritage\", \"imc_urban\", \"imc_phototourism\", \"imc2024\", \"KITTI\", \"euroc\", \"eth3d_raw_dslr\", \"blendedmvs\"]\n",
    "roots_abs = [\"lamar_map\", \"eth3d_slam\", \"strecha\"]\n",
    "\n",
    "results_full = [[] for i in range(len(path_bases))]\n",
    "results_table = []\n",
    "for root in roots:\n",
    "    if root in roots_abs:\n",
    "        mode = \"abs\"\n",
    "    elif root in roots_rel:\n",
    "        mode = \"rel\"\n",
    "    else:\n",
    "        print(\"NOT KNOWN!!!\")\n",
    "    \n",
    "    # mode = 'abs'\n",
    "    if mode == \"rel\":\n",
    "        if root.find(\"eth3d_dslr\") >= 0:\n",
    "            threshold = 1\n",
    "            AUC_thres =  [1, 3, 5]\n",
    "            # threshold = 0.02\n",
    "            # AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root == \"eth3d_rig\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 3, 5]\n",
    "            # threshold = 0.02\n",
    "            # AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root == \"eth3d_slam\":\n",
    "            # threshold = 1\n",
    "            # AUC_thres = [3, 5, 10]\n",
    "            threshold = 0.02\n",
    "            AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root.find(\"imc\") >= 0:\n",
    "            threshold = 3\n",
    "            AUC_thres = [3, 5, 10]\n",
    "        elif root == \"mip360\":\n",
    "            threshold = 3\n",
    "            AUC_thres = [3, 5, 10]\n",
    "        elif root == \"KITTI\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [0.5, 1, 2]\n",
    "        elif root == \"euroc\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [0.5, 1, 2]\n",
    "        else:\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 3, 5]\n",
    "        idx = list(range(len(AUC_thres) + 2))[1:]\n",
    "    elif mode == \"abs\":\n",
    "        if root == \"lamar_map\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 5]\n",
    "        elif root == \"eth3d_slam\":\n",
    "            threshold = 0.1\n",
    "            AUC_thres = [0.1, 0.5]\n",
    "        else:\n",
    "            threshold = 0.1\n",
    "            AUC_thres = [0.1, 0.5]\n",
    "        idx = list(range(len(AUC_thres) + 2))\n",
    "    print(threshold, AUC_thres)\n",
    "\n",
    "    results_class = {}\n",
    "    for dataset in datasets_full[root]:\n",
    "        if root == \"eth3d_slam\":\n",
    "            prefix = dataset.split(\"_\")[0]\n",
    "        else:\n",
    "            prefix = dataset\n",
    "        # prefix = dataset\n",
    "\n",
    "        if prefix not in results_class:\n",
    "            results_class[prefix] = {path_base: [] for path_base in path_bases}\n",
    "\n",
    "        result_row = []\n",
    "        times_row = []\n",
    "        for i, path_base in enumerate(path_bases):\n",
    "            if path_base is None:\n",
    "                continue\n",
    "            if results[root][dataset][i] is None:\n",
    "                continue\n",
    "            inliers = results[root][dataset][i] < threshold\n",
    "\n",
    "            row = []\n",
    "            row.append(np.average(results[root][dataset][i] < threshold) * 100)\n",
    "            row += compute_auc(results[root][dataset][i], AUC_thres)\n",
    "            result_row.append(row)\n",
    "\n",
    "            if path_base == \"colmap\" or path_base == \"colmap/0\" or path_base.find(\"sparse\") >= 0:\n",
    "                if dataset in times_colmap[root]:\n",
    "                    result_row[-1].append(times_colmap[root][dataset])\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "                # result_row[-1].append(times_colmap[root][dataset])\n",
    "            elif path_base.find(\"theia\") != -1:\n",
    "                if dataset in times_theia:\n",
    "                    result_row[-1].append(times_theia[dataset])\n",
    "                elif dataset in times_theia[root]:\n",
    "                    result_row[-1].append(times_theia[root][dataset])\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            elif path_base.find(\"openmvg\") != -1:\n",
    "                result_row[-1].append(times_openmvg[dataset])\n",
    "            elif path_base.find(\"ligt\") != -1:\n",
    "                result_row[-1].append(times_ligt[dataset])\n",
    "            elif post_fixes[i] is not None:\n",
    "                if post_fixes[i] in times_3d:\n",
    "                    if dataset in times_3d[post_fixes[i]][root]:\n",
    "                        result_row[-1].append(times_3d[post_fixes[i]][root][dataset])\n",
    "                    elif dataset in times_3d[post_fixes[i]]:\n",
    "                        result_row[-1].append(times_3d[post_fixes[i]][dataset])\n",
    "                    else:\n",
    "                        result_row[-1].append(1e10)\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            else:\n",
    "                if post_fix in times_3d:\n",
    "                    if dataset in times_3d[post_fix][root]:\n",
    "                        result_row[-1].append(times_3d[post_fix][root][dataset])\n",
    "                    else:\n",
    "                        result_row[-1].append(1e10)\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            \n",
    "            results_class[prefix][path_base].append(result_row[-1])\n",
    "            results_full[i].append(result_row[-1])\n",
    "\n",
    "        # result_row = np.asarray(result_row)\n",
    "        # result_row = np.round(result_row, 2)\n",
    "    # print(results_class)\n",
    "    \n",
    "    prefix_full = sorted(list(results_class.keys()))\n",
    "    for prefix in prefix_full:\n",
    "        print(prefix.replace(\"_\", \"\\\\_\"), end=\" \")\n",
    "        result_row = []\n",
    "        max_rows = np.max([len(results_class[prefix][path_base]) for path_base in path_bases])\n",
    "        for path_base in path_bases:\n",
    "            if len(results_class[prefix][path_base]) == 0:\n",
    "                result_row.append(np.zeros((len(AUC_thres) + 2,)))\n",
    "                result_row[-1][-1] = 1e10\n",
    "                continue\n",
    "            result_row.append(np.sum(np.asarray(results_class[prefix][path_base]), axis=0) / max_rows)\n",
    "            result_row[-1][-1] = np.mean(np.asarray(results_class[prefix][path_base])[:,-1], axis=0)\n",
    "        \n",
    "\n",
    "        result_row = np.asarray(result_row)\n",
    "        result_row = np.round(result_row, 1)\n",
    "\n",
    "        ranks = [rankify(result_row[:, i], 1) for i in range(result_row.shape[1])]\n",
    "        ranks[-1] = rankify(result_row[:, -1], -1)\n",
    "\n",
    "        \n",
    "        for j in idx[:-1]:\n",
    "            print(\"&\", end=\"\")\n",
    "            counter = 0\n",
    "            for i, path_base in enumerate(path_bases):\n",
    "                if prefix in results[root] and results[root][prefix][i] is None:\n",
    "                    print(\"& -\", end=\" \")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                    \n",
    "                if ranks[j][counter] == 0:\n",
    "                    print(\"& \\\\cellcolor{tabfirst}\" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "                elif ranks[j][counter] == 1:\n",
    "                    print(\"& \\\\cellcolor{tabsecond}\" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "                # elif ranks[j][counter] == 2:\n",
    "                #     print(\"& \\\\cellcolor{tabthird}\" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "\n",
    "                # if ranks[j][counter] == 0:\n",
    "                #     print(\"& \\\\textbf{\" + \"{:.1f}\".format(result_row[i][j]) + \"}\", end=\" \")\n",
    "                # elif ranks[j][counter] == 1:\n",
    "                #     print(\"& \\\\textit{\" + \"{:.1f}\".format(result_row[i][j]) + \"}\", end=\" \")\n",
    "                # elif ranks[j][counter] == 2:\n",
    "                #     print(\"& \\\\cellcolor{tabthird}\" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "                else:\n",
    "                    print(\"& \" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "                counter += 1\n",
    "        print(\"\\\\\\\\\")\n",
    "                \n",
    "        # for j in idx:\n",
    "        #     counter = 0\n",
    "        #     for i, path_base in enumerate(path_bases):\n",
    "        #         if prefix in results[root] and results[root][prefix][i] is None:\n",
    "        #             print(\"& -\", end=\" \")\n",
    "        #             counter += 1\n",
    "        #             continue\n",
    "                    \n",
    "        #         print(\" \" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "        #         counter += 1\n",
    "            \n",
    "        # print()\n",
    "        \n",
    "\n",
    "result_full_average = []\n",
    "result_full_average_valid = []\n",
    "results_test = []\n",
    "\n",
    "total_dataset = 0\n",
    "for root in roots:\n",
    "    total_dataset += len(datasets_full[root])\n",
    "for i in range(len(path_bases)):\n",
    "    if len(results_full[i]) == 0:\n",
    "        result_full_average.append(np.nan * np.zeros((len(idx),)))\n",
    "        continue\n",
    "\n",
    "    results_test.append(np.asarray(results_full[i])[0])\n",
    "\n",
    "    result_full_np = np.asarray(results_full[i])\n",
    "    result_full_average.append(np.sum(result_full_np, axis=0) / total_dataset)\n",
    "    result_full_average[-1][-1] = np.mean(result_full_np[:, -1])\n",
    "    result_full_average_valid.append(np.mean(result_full_np, axis=0))\n",
    "\n",
    "result_full_average = np.asarray(result_full_average)\n",
    "result_full_average_valid = np.asarray(result_full_average_valid)\n",
    "\n",
    "result_full_average = np.round(result_full_average, 1)\n",
    "result_full_average_valid = np.round(result_full_average_valid, 1)\n",
    "ranks_ave = [rankify(result_full_average[:, i], 1) for i in range(result_full_average.shape[1])]\n",
    "ranks_ave[-1] = rankify(result_full_average[:, -1], -1)\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(\"\\\\textit{Average}\", end=\" \")\n",
    "\n",
    "for j in idx:\n",
    "    print(\"&\", end=\"\")\n",
    "    counter = 0\n",
    "    for i, path_base in enumerate(path_bases):\n",
    "        if np.isnan(result_full_average[i][0]):\n",
    "            print(\"& \" + \"-\", end=\" \")\n",
    "            counter += 1\n",
    "            continue\n",
    "        if ranks_ave[j][counter] == 0:\n",
    "            print(\"& \\\\cellcolor{tabfirst}\" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        elif ranks_ave[j][counter] == 1:\n",
    "            print(\"& \\\\cellcolor{tabsecond}\" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        # elif ranks_ave[j][counter] == 2:\n",
    "        #     print(\"& \\\\cellcolor{tabthird}\" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        # if ranks_ave[j][counter] == 0:\n",
    "        #     print(\"& \\\\textbf{\" + \"{:.1f}\".format(result_full_average[i][j]) + \"}\", end=\" \")\n",
    "        # elif ranks_ave[j][counter] == 1:\n",
    "        #     print(\"& \\\\t\n",
    "        # extit{\" + \"{:.1f}\".format(result_full_average[i][j]) + \"}\", end=\" \")\n",
    "        else:\n",
    "            print(\"& \" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        # print(\"& \" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")delivery\\_area && \\cellcolor{tabsecond}92.3 & \\cellcolor{tabfirst}92.8 & 87.6 && \\cellcolor{tabsecond}97.4 & \\cellcolor{tabfirst}97.6 & 95.9 && \\cellcolor{tabsecond}98.5 & \\cellcolor{tabfirst}98.6 & 97.5 \\\\\n",
    "\n",
    "            \n",
    "        counter += 1\n",
    "print(\"\\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for SLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIN\n"
     ]
    }
   ],
   "source": [
    "realign = True\n",
    "\n",
    "results = {}\n",
    "results_r = {}\n",
    "total_image = {}\n",
    "errors_r_all = []\n",
    "errors_t_all = []\n",
    "\n",
    "for root in roots:\n",
    "    results[root] = {}\n",
    "    results_r[root] = {}\n",
    "    total_image[root] = {}\n",
    "    for dataset in datasets_full[root]:\n",
    "        dir_base = path_roots[root] + dataset + \"/\"\n",
    "        if root == \"strecha\":\n",
    "            dir_base = path_roots[root] + dataset + \"/colmap/\"\n",
    "        \n",
    "        print(dataset)\n",
    "\n",
    "        results[root][dataset] = []\n",
    "        results_r[root][dataset] = []\n",
    "        database_path = os.path.join(dir_base, \"database.db\")\n",
    "        image_path = os.path.join(dir_base, \"images\")\n",
    "\n",
    "        for path_base in path_bases:\n",
    "            if path_base is None:\n",
    "                results[root][dataset].append(None)\n",
    "                results_r[root][dataset].append(None)\n",
    "                continue\n",
    "            \n",
    "            if not os.path.exists(os.path.join(dir_base, path_base, \"cameras.txt\")) and not os.path.exists(os.path.join(dir_base, path_base, \"cameras.bin\")):\n",
    "                print(os.path.join(dir_base, path_base), \"not possible\")\n",
    "                results[root][dataset].append(None)\n",
    "                results_r[root][dataset].append(None)\n",
    "                continue\n",
    "            \n",
    "            status = True\n",
    "            if path_base.find(\"sparse/0\") >= 0:\n",
    "                total_dirs = sorted([x for x in os.listdir(dir_base + \"sparse\") if os.path.isdir(dir_base + \"sparse/\" + x)])\n",
    "                if len(total_dirs) > 1:\n",
    "                    # num_imgs = []\n",
    "                    max_num_img = 0\n",
    "                    max_idx = -1\n",
    "                    for idx, sub_dir in enumerate(total_dirs):\n",
    "                        reconstruction = pycolmap.Reconstruction()\n",
    "                        reconstruction.read(dir_base + \"sparse/\" + sub_dir)\n",
    "                        if reconstruction.num_images() > max_num_img:\n",
    "                            max_num_img = reconstruction.num_images()\n",
    "                            max_idx = idx\n",
    "                    if max_idx != 0:\n",
    "                        status = False\n",
    "                        path_base = \"sparse/\" + str(max_idx)\n",
    "                        print(\"path_base:\", path_base)\n",
    "            \n",
    "            file = open(dir_base + \"centers_gt.txt\", \"r\")\n",
    "            centers_gt = {}\n",
    "            names = []\n",
    "            centers = []\n",
    "            for line in file:\n",
    "                line = line.split()\n",
    "                centers_gt[line[0]] = np.array([float(line[1]), float(line[2]), float(line[3])])\n",
    "                names.append(line[0])\n",
    "                centers.append(centers_gt[line[0]])\n",
    "\n",
    "            if realign:\n",
    "                for name in reconstruction_file_names:\n",
    "                    if os.path.exists(os.path.join(dir_base, path_base, name)) and os.path.exists(os.path.join(dir_base, path_base, name[:-4]+ \".txt\")):\n",
    "                        os.remove(os.path.join(dir_base, path_base, name))\n",
    "                        print(\"removed\", os.path.join(dir_base, path_base, name))\n",
    "\n",
    "                if path_base.find(\"colmap_openmvg\") >= 0 or path_base.find(\"colmap_ligt\") >= 0:\n",
    "                    reconstruction = pycolmap.Reconstruction()\n",
    "                    reconstruction.read_text(dir_base + path_base)\n",
    "                    for id in reconstruction.reg_image_ids():\n",
    "                        image_name = reconstruction.images[id].name\n",
    "                        if image_name.find(\"records_data/\") >= 0:\n",
    "                            reconstruction.images[id].name = image_name.replace(\"records_data/\", \"\")\n",
    "                    reconstruction.write_text(dir_base + path_base)\n",
    "                \n",
    "\n",
    "                opt = pycolmap.RANSACOptions()\n",
    "                if root == \"1dsfm_colmap\":\n",
    "                    opt.max_error = 0.5\n",
    "                elif root == \"KITTI\":\n",
    "                    opt.max_error = 5.0\n",
    "                elif root == \"1dsfm\":\n",
    "                    opt.max_error = 10.\n",
    "                elif root == \"lamar_map\":\n",
    "                    opt.max_error = 1.\n",
    "                else:\n",
    "                    opt.max_error = 0.1\n",
    "                try:\n",
    "                    reconstruction = pycolmap.Reconstruction(dir_base + path_base)\n",
    "                    trans = pycolmap.align_reconstrution_to_locations(reconstruction, names, centers, 3, opt)\n",
    "                    reconstruction.transform(trans)\n",
    "                    reconstruction.write_text(dir_base + path_base)\n",
    "                except:\n",
    "                    print(\"Error in aligning\")\n",
    "                    results[root][dataset].append(None)\n",
    "                    results_r[root][dataset].append(None)\n",
    "                    continue\n",
    "\n",
    "            # for i in range(len(centers)):\n",
    "            #     print(centers_gt[names[i]], trans centers[i])\n",
    "\n",
    "            file = open(dir_base + \"gtpose_trans.txt\", \"r\")\n",
    "            poses_gt = {}\n",
    "            for line in file:\n",
    "                line = line.split()\n",
    "                poses_gt[line[0]] = poselib.CameraPose()\n",
    "                poses_gt[line[0]].q = np.array([float(line[1]), float(line[2]), float(line[3]), float(line[4])])\n",
    "                poses_gt[line[0]].t = np.array([float(line[5]), float(line[6]), float(line[7])])\n",
    "\n",
    "                \n",
    "            reconstruction = pycolmap.Reconstruction()\n",
    "            reconstruction.read_text(dir_base + path_base)\n",
    "            # centers_est = {}\n",
    "            errors_r = []\n",
    "            errors_t = []\n",
    "            pose = poselib.CameraPose()\n",
    "            for image_name in centers_gt.keys():\n",
    "                image = reconstruction.find_image_with_name(image_name)\n",
    "                if image is not None:\n",
    "                    center = image.projection_center()\n",
    "                    errors_t.append(np.linalg.norm(center - centers_gt[image_name]))\n",
    "                    pose.q = image.cam_from_world.rotation.quat[[3,0,1,2]]\n",
    "                    pose.t = image.cam_from_world.translation\n",
    "                    # errors_t.append(np.linalg.norm(pose.t - poses_gt[image_name].t))\n",
    "                    errors_r.append(calc_angle(pose, poses_gt[image_name]))\n",
    "                else:\n",
    "                    errors_t.append(100)\n",
    "                    errors_r.append(180)\n",
    "            \n",
    "\n",
    "            errors_r_all += errors_r\n",
    "            errors_t_all += errors_t\n",
    "            errors_t = np.array(errors_t)\n",
    "            errors_r = np.array(errors_r)\n",
    "            results[root][dataset].append(errors_t)\n",
    "            results_r[root][dataset].append(errors_r)\n",
    "            total_image[root][dataset] = len(centers_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 3, 5]\n",
      "&& \\textbf{33.9} "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mranks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m& \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtextbf\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(result_row[i][j]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# printing method, color-coded (for transposed)\n",
    "\n",
    "roots_rel = [\"eth3d_rig\", \"mip360\", \"eth3d_dslr\", \"eth3d_dslr_test\", \"imc_haiper\", \"imc_heritage\", \"imc_urban\", \"imc_phototourism\", \"imc2024\"]\n",
    "roots_abs = [\"lamar_map\", \"eth3d_slam\", \"strecha\"]\n",
    "\n",
    "results_full = [[] for i in range(len(path_bases))]\n",
    "results_table = []\n",
    "for root in roots:\n",
    "    if root in roots_abs:\n",
    "        mode = \"abs\"\n",
    "    elif root in roots_rel:\n",
    "        mode = \"rel\"\n",
    "    else:\n",
    "        print(\"NOT KNOWN!!!\")\n",
    "    \n",
    "    # mode = 'abs'\n",
    "    if mode == \"rel\":\n",
    "        if root.find(\"eth3d_dslr\") >= 0:\n",
    "            threshold = 1\n",
    "            AUC_thres =  [1, 3, 5]\n",
    "            # threshold = 0.02\n",
    "            # AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root == \"eth3d_rig\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 3, 5]\n",
    "            # threshold = 0.02\n",
    "            # AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root == \"eth3d_slam\":\n",
    "            # threshold = 1\n",
    "            # AUC_thres = [3, 5, 10]\n",
    "            threshold = 0.02\n",
    "            AUC_thres =  [0.02, 0.05, 0.1]\n",
    "        elif root.find(\"imc\") >= 0:\n",
    "            threshold = 3\n",
    "            AUC_thres = [3, 5, 10]\n",
    "        elif root == \"mip360\":\n",
    "            threshold = 3\n",
    "            AUC_thres = [3, 5, 10]\n",
    "        else:\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 3, 5]\n",
    "        idx = list(range(len(AUC_thres) + 2))[1:]\n",
    "    elif mode == \"abs\":\n",
    "        if root == \"lamar_map\":\n",
    "            threshold = 1\n",
    "            AUC_thres = [1, 5]\n",
    "        elif root == \"eth3d_slam\":\n",
    "            threshold = 0.1\n",
    "            AUC_thres = [0.1, 0.5]\n",
    "        else:\n",
    "            threshold = 0.1\n",
    "            AUC_thres = [0.1, 0.5]\n",
    "        idx = list(range(len(AUC_thres) + 2))\n",
    "    print(threshold, AUC_thres)\n",
    "\n",
    "    results_class = {}\n",
    "    for dataset in datasets_full[root]:\n",
    "        if root == \"eth3d_slam\":\n",
    "            prefix = dataset.split(\"_\")[0]\n",
    "        else:\n",
    "            prefix = dataset\n",
    "        # prefix = dataset\n",
    "\n",
    "        if prefix not in results_class:\n",
    "            results_class[prefix] = {path_base: [] for path_base in path_bases}\n",
    "\n",
    "        result_row = []\n",
    "        times_row = []\n",
    "        for i, path_base in enumerate(path_bases):\n",
    "            if path_base is None:\n",
    "                continue\n",
    "            if results[root][dataset][i] is None:\n",
    "                continue\n",
    "            inliers = results[root][dataset][i] < threshold\n",
    "\n",
    "            row = []\n",
    "            row.append(np.average(results[root][dataset][i] < threshold) * 100)\n",
    "            row += compute_auc(results[root][dataset][i], AUC_thres)\n",
    "            result_row.append(row)\n",
    "\n",
    "            if path_base == \"colmap\" or path_base == \"colmap/0\" or path_base.find(\"sparse\") >= 0:\n",
    "                if dataset in times_colmap[root]:\n",
    "                    result_row[-1].append(times_colmap[root][dataset])\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "                # result_row[-1].append(times_colmap[root][dataset])\n",
    "            elif path_base.find(\"theia\") != -1:\n",
    "                if dataset in times_theia:\n",
    "                    result_row[-1].append(times_theia[dataset])\n",
    "                elif dataset in times_theia[root]:\n",
    "                    result_row[-1].append(times_theia[root][dataset])\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            elif path_base.find(\"hsfm\") != -1:\n",
    "                if dataset in times_hsfm:\n",
    "                    result_row[-1].append(times_hsfm[dataset])\n",
    "                elif dataset in times_hsfm[root]:\n",
    "                    result_row[-1].append(times_hsfm[root][dataset])\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            elif path_base.find(\"openmvg\") != -1:\n",
    "                result_row[-1].append(times_openmvg[dataset])\n",
    "            elif path_base.find(\"ligt\") != -1:\n",
    "                result_row[-1].append(times_ligt[dataset])\n",
    "            elif post_fixes[i] is not None:\n",
    "                if post_fixes[i] in times_3d:\n",
    "                    if dataset in times_3d[post_fixes[i]][root]:\n",
    "                        result_row[-1].append(times_3d[post_fixes[i]][root][dataset])\n",
    "                    elif dataset in times_3d[post_fixes[i]]:\n",
    "                        result_row[-1].append(times_3d[post_fixes[i]][dataset])\n",
    "                    else:\n",
    "                        result_row[-1].append(1e10)\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            else:\n",
    "                if post_fix in times_3d:\n",
    "                    if dataset in times_3d[post_fix][root]:\n",
    "                        result_row[-1].append(times_3d[post_fix][root][dataset])\n",
    "                    else:\n",
    "                        result_row[-1].append(1e10)\n",
    "                else:\n",
    "                    result_row[-1].append(1e10)\n",
    "            \n",
    "            results_class[prefix][path_base].append(result_row[-1])\n",
    "            results_full[i].append(result_row[-1])\n",
    "\n",
    "        # result_row = np.asarray(result_row)\n",
    "        # result_row = np.round(result_row, 2)\n",
    "    # print(results_class)\n",
    "    \n",
    "    prefix_full = sorted(list(results_class.keys()))\n",
    "    for prefix in prefix_full:\n",
    "        # print(prefix.replace(\"_\", \"\\\\_\"), end=\" \")\n",
    "        result_row = []\n",
    "        max_rows = np.max([len(results_class[prefix][path_base]) for path_base in path_bases])\n",
    "        for path_base in path_bases:\n",
    "            if len(results_class[prefix][path_base]) == 0:\n",
    "                result_row.append(np.zeros((len(AUC_thres) + 2,)))\n",
    "                result_row[-1][-1] = 1e10\n",
    "                continue\n",
    "            result_row.append(np.sum(np.asarray(results_class[prefix][path_base]), axis=0) / max_rows)\n",
    "            result_row[-1][-1] = np.mean(np.asarray(results_class[prefix][path_base])[:,-1], axis=0)\n",
    "        \n",
    "\n",
    "        result_row = np.asarray(result_row)\n",
    "        result_row = np.round(result_row, 1)\n",
    "\n",
    "        ranks = [rankify(result_row[:, i], 1) for i in range(result_row.shape[1])]\n",
    "        ranks[-1] = rankify(result_row[:, -1], -1)\n",
    "\n",
    "        \n",
    "        # for i, path_base in enumerate(path_bases):\n",
    "        #     print(\"&\", end=\"\")\n",
    "        #     counter = 0\n",
    "        #     for j in idx:\n",
    "        #         if prefix in results[root] and results[root][prefix][i] is None:\n",
    "        #             print(\"& -\", end=\" \")\n",
    "        #             counter += 1\n",
    "        #             continue\n",
    "                    \n",
    "        #         if ranks[j][counter] == 0:\n",
    "        #             print(\"& \\\\textbf{\" + \"{:.1f}\".format(result_row[i][j]) + \"}\", end=\" \")\n",
    "        #         else:\n",
    "        #             print(\"& \" + \"{:.1f}\".format(result_row[i][j]), end=\" \")\n",
    "        #         counter += 1\n",
    "            \n",
    "        # print(\"\\\\\\\\\")\n",
    "        \n",
    "\n",
    "result_full_average = []\n",
    "result_full_average_valid = []\n",
    "results_test = []\n",
    "\n",
    "total_dataset = 0\n",
    "for root in roots:\n",
    "    print(root)\n",
    "    total_dataset += len(datasets_full[root])\n",
    "for i in range(len(path_bases)):\n",
    "    if len(results_full[i]) == 0:\n",
    "        result_full_average.append(np.nan * np.zeros((len(idx),)))\n",
    "        continue\n",
    "\n",
    "    results_test.append(np.asarray(results_full[i])[0])\n",
    "\n",
    "    result_full_np = np.asarray(results_full[i])\n",
    "    result_full_average.append(np.sum(result_full_np, axis=0) / total_dataset)\n",
    "    result_full_average[-1][-1] = np.mean(result_full_np[:, -1])\n",
    "    result_full_average_valid.append(np.mean(result_full_np, axis=0))\n",
    "\n",
    "result_full_average = np.asarray(result_full_average)\n",
    "result_full_average_valid = np.asarray(result_full_average_valid)\n",
    "\n",
    "result_full_average = np.round(result_full_average, 1)\n",
    "result_full_average_valid = np.round(result_full_average_valid, 1)\n",
    "ranks_ave = [rankify(result_full_average[:, i], 1) for i in range(result_full_average.shape[1])]\n",
    "ranks_ave[-1] = rankify(result_full_average[:, -1], -1)\n",
    "\n",
    "\n",
    "for i, path_base in enumerate(path_bases):\n",
    "    print(path_base, end=\" \")\n",
    "    print(\"&\", end=\"\")\n",
    "    for j in idx:\n",
    "        if np.isnan(result_full_average[i][j]):\n",
    "            print(\"& \" + \"-\", end=\" \")\n",
    "            continue\n",
    "        # if ranks_ave[j][i] == 0:\n",
    "        #     print(\"& \\\\textbf{\" + \"{:.1f}\".format(result_full_average[i][j]) + \"}\", end=\" \")\n",
    "\n",
    "        # if ranks_ave[j][i] == 0:\n",
    "        #     print(\"& \\\\cellcolor{tabfirst}\" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        # elif ranks_ave[j][i] == 1:\n",
    "        #     print(\"& \\\\cellcolor{tabsecond}\" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        # else:\n",
    "        #     print(\"& \" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "        print(\"& \" + \"{:.1f}\".format(result_full_average[i][j]), end=\" \")\n",
    "    print(\"\\\\\\\\\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle && 23.01 & 17.75 & \\cellcolor{tabsecond}23.13 & \\cellcolor{tabfirst}23.15 && 0.526 & 0.352 & \\cellcolor{tabsecond}0.531 & \\cellcolor{tabfirst}0.532 \\\\\n",
      "bonsai && 23.88 & 28.54 & \\cellcolor{tabfirst}30.36 & \\cellcolor{tabsecond}29.66 && 0.767 & 0.872 & \\cellcolor{tabfirst}0.904 & \\cellcolor{tabsecond}0.896 \\\\\n",
      "counter && 26.76 & \\cellcolor{tabsecond}26.78 & 26.72 & \\cellcolor{tabfirst}26.81 && 0.835 & \\cellcolor{tabsecond}0.836 & 0.835 & \\cellcolor{tabfirst}0.837 \\\\\n",
      "garden && \\cellcolor{tabsecond}24.97 & 20.19 & \\cellcolor{tabsecond}24.97 & \\cellcolor{tabfirst}24.98 && \\cellcolor{tabsecond}0.653 & 0.456 & \\cellcolor{tabfirst}0.655 & \\cellcolor{tabsecond}0.653 \\\\\n",
      "kitchen && \\cellcolor{tabsecond}29.32 & 29.02 & \\cellcolor{tabfirst}29.35 & 29.23 && \\cellcolor{tabsecond}0.853 & 0.841 & \\cellcolor{tabfirst}0.855 & 0.851 \\\\\n",
      "room && 19.11 & 17.07 & \\cellcolor{tabfirst}29.41 & \\cellcolor{tabsecond}29.14 && 0.691 & 0.643 & \\cellcolor{tabfirst}0.876 & \\cellcolor{tabsecond}0.871 \\\\\n",
      "stump && 23.56 & 19.43 & \\cellcolor{tabsecond}23.81 & \\cellcolor{tabfirst}23.98 && 0.584 & 0.408 & \\cellcolor{tabsecond}0.595 & \\cellcolor{tabfirst}0.602 \\\\\n",
      "\\midrule\n",
      "Average && 24.37 & 22.68 & \\cellcolor{tabfirst}26.82 & \\cellcolor{tabsecond}26.71 && 0.701 & 0.630 & \\cellcolor{tabfirst}0.750 & \\cellcolor{tabsecond}0.749 \\\\\n"
     ]
    }
   ],
   "source": [
    "# data = np.loadtxt(\"../../../instant-ngp/scripts/results_arxiv.txt\")\n",
    "data = np.loadtxt(\"../../../instant-ngp/scripts/results_final.txt\")\n",
    "\n",
    "datasets = datasets_full['mip360']\n",
    "\n",
    "# openmvg, glomap, theia, colmap\n",
    "# methods = [\"theia\"]\n",
    "methods = [\"openmvg\", \"theia\", \"glomap\", \"colmap\"]\n",
    "\n",
    "\n",
    "results_psnr = []\n",
    "results_ssim = []\n",
    "# = data[:]\n",
    "\n",
    "# for i, dataset in enumerate(datasets):\n",
    "for i, method in enumerate(methods):\n",
    "    results_psnr.append(data[i::len(methods), 0])\n",
    "    results_ssim.append(data[i::len(methods), 1])\n",
    "\n",
    "results_psnr = np.array(results_psnr).T\n",
    "results_ssim = np.array(results_ssim).T\n",
    "\n",
    "results = [results_psnr, results_ssim]\n",
    "results[0] = np.concatenate([results[0], np.mean(results_psnr, axis=0, keepdims=True)], axis=0)\n",
    "results[1] = np.concatenate([results[1], np.mean(results_ssim, axis=0, keepdims=True)], axis=0)\n",
    "\n",
    "precisions = [2, 3]\n",
    "\n",
    "# bold a number if is the largest in the row\n",
    "for i, dataset in enumerate(datasets + [\"Average\"]):\n",
    "    print(dataset, end=\" \")\n",
    "    results[0][i] = np.round(results[0][i], precisions[0])\n",
    "    results[1][i] = np.round(results[1][i], precisions[1])\n",
    "    ranks = [rankify(results[0][i], 1), rankify(results[1][i], 1)]\n",
    "    for k in range(2):\n",
    "        print(\"&\", end=\"\")\n",
    "        for j, method in enumerate(methods):\n",
    "            # print(results[0])\n",
    "            print(\"& \", end=\"\")\n",
    "            if ranks[k][j] == 0:\n",
    "                print(\"\\\\cellcolor{tabfirst}\", end=\"\")\n",
    "            elif ranks[k][j] == 1:\n",
    "                print(\"\\\\cellcolor{tabsecond}\", end=\"\")\n",
    "            print((\"{:\" + f'.{precisions[k]}f' + \"}\").format(results[k][i, j]), end=\" \")\n",
    "    \n",
    "    print(\"\\\\\\\\\")\n",
    "    if i == len(datasets) - 1:\n",
    "        print(\"\\\\midrule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herzjesu 4.13\n",
      "Herzjesu_large 5.4\n",
      "Fountain 2.79\n",
      "Entry 6.32\n",
      "Castle 24.95\n",
      "Castle_large 22.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in [\"Herzjesu\", \"Herzjesu_large\", \"Fountain\", \"Entry\", \"Castle\", \"Castle_large\"]:\n",
    "    if root == \"eth3d_slam\":\n",
    "        prefix = dataset.split(\"_\")[0]\n",
    "    else:\n",
    "        prefix = dataset\n",
    "    # prefix = dataset\n",
    "\n",
    "    if prefix not in results_class:\n",
    "        results_class[prefix] = {path_base: [] for path_base in path_bases}\n",
    "\n",
    "    result_row = []\n",
    "    times_row = []\n",
    "    for i, path_base in enumerate(path_bases):\n",
    "        if path_base is None:\n",
    "            continue\n",
    "        if results[root][dataset][i] is None:\n",
    "            continue\n",
    "        inliers = results[root][dataset][i] < threshold\n",
    "\n",
    "        print(dataset, np.round(np.average(results[root][dataset][i] * 1000), 2))\n",
    "\n",
    "        # row = []\n",
    "        # row.append(np.average(results[root][dataset][i] < threshold) * 100)\n",
    "        # row += compute_auc(results[root][dataset][i], AUC_thres)\n",
    "        # result_row.append(row)\n",
    "\n",
    "        \n",
    "        # results_class[prefix][path_base].append(result_row[-1])\n",
    "        # results_full[i].append(result_row[-1])\n",
    "\n",
    "    # result_row = np.asarray(result_row)\n",
    "    # result_row = np.round(result_row, 2)\n",
    "# print(results_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
